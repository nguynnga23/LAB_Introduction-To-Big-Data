{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "from pymongo import MongoClient\n",
    "from sqlalchemy import create_engine, text, Table, MetaData\n",
    "from sqlalchemy.dialects.postgresql import insert\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_email(email_text):\n",
    "    try:\n",
    "        return {\n",
    "            \"Message-ID\": re.search(r\"Message-ID:\\s*<(.+?)>\", email_text).group(1),\n",
    "            \"Date\": re.search(r\"Date:\\s*(.+?)\\n\", email_text).group(1),\n",
    "            \"From\": re.search(r\"From:\\s*(.+?)\\n\", email_text).group(1),\n",
    "            \"To\": re.search(r\"To:\\s*(.+?)\\n\", email_text, re.DOTALL).group(1).replace(\"\\n\\t\", \"\"),\n",
    "            \"Subject\": re.search(r\"Subject:\\s*(.+?)\\n\", email_text).group(1),\n",
    "            \"Cc\": re.search(r\"Cc:\\s*(.+?)\\n\", email_text, re.DOTALL).group(1).replace(\"\\n\\t\", \"\") if re.search(r\"Cc:\\s*(.+?)\\n\", email_text) else None,\n",
    "            \"Bcc\": re.search(r\"Bcc:\\s*(.+?)\\n\", email_text, re.DOTALL).group(1).replace(\"\\n\\t\", \"\") if re.search(r\"Bcc:\\s*(.+?)\\n\", email_text) else None,\n",
    "            \"X-From\": re.search(r\"X-From:\\s*(.+?)\\n\", email_text).group(1),\n",
    "            \"X-To\": re.search(r\"X-To:\\s*(.+?)\\n\", email_text, re.DOTALL).group(1).replace(\"\\n\\t\", \"\"),\n",
    "            \"X-cc\": re.search(r\"X-cc:\\s*(.+?)\\n\", email_text, re.DOTALL).group(1).replace(\"\\n\\t\", \"\") if re.search(r\"X-cc:\\s*(.+?)\\n\", email_text) else None,\n",
    "            \"X-bcc\": re.search(r\"X-bcc:\\s*(.*)\", email_text).group(1) if re.search(r\"X-bcc:\\s*(.*)\", email_text) else None,\n",
    "            \"X-Folder\": re.search(r\"X-Folder:\\s*(.+?)\\n\", email_text).group(1),\n",
    "            \"X-Origin\": re.search(r\"X-Origin:\\s*(.+?)\\n\", email_text).group(1),\n",
    "            \"X-FileName\": re.search(r\"X-FileName:\\s*(.+?)\\n\", email_text).group(1),\n",
    "            \"Body\": email_text.split(\"\\n\\n\", 1)[1].strip() if \"\\n\\n\" in email_text else None\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing email: {e}\")\n",
    "        return None\n",
    "    \n",
    "def normalize_column_names(columns):\n",
    "    normalized_columns = []\n",
    "    for col in columns:\n",
    "        col = col.strip()\n",
    "        col = col.lower()\n",
    "        col = re.sub(r'[^\\w\\s]', '', col)\n",
    "        col = re.sub(r'\\s+', '_', col)\n",
    "        normalized_columns.append(col)\n",
    "    return normalized_columns\n",
    "def extract_csv(file_dir, batch_size):\n",
    "    \n",
    "    data = pd.read_csv(file_dir).sample(batch_size)\n",
    "    return data\n",
    "    \n",
    "def mongo_job(file_dir, batch_size, mongo_info):\n",
    "    \n",
    "    data = extract_csv(file_dir, batch_size)\n",
    "    data = data.to_dict(orient='records')\n",
    "    print('Read and converted csv successfully')\n",
    "    \n",
    "    client = MongoClient(mongo_info['client'])\n",
    "    db = client[mongo_info['db']] \n",
    "    collection = db[mongo_info['coll']]\n",
    "    collection.insert_many(data)\n",
    "    print('Load data into mongo successfully')\n",
    "    \n",
    "def postgres_job(mongo_info, ps_info):\n",
    "    \n",
    "    client = MongoClient(mongo_info['client'])\n",
    "    db = client[mongo_info['db']] \n",
    "    collection = db[mongo_info['coll']]\n",
    "    print('Accessed mongo successfully')\n",
    "    \n",
    "    documents = collection.find()\n",
    "    documents = list(documents)\n",
    "    print('Extracted from mongo successfully')\n",
    "    \n",
    "    df_doc = pd.DataFrame(documents)\n",
    "    df_doc.drop_duplicates('file',keep='first',inplace=True)\n",
    "    df_doc = df_doc['message'].apply(parse_email).apply(pd.Series)\n",
    "    df_doc.columns = normalize_column_names(df_doc.columns)\n",
    "    df_doc['date'] = pd.to_datetime(df_doc['date'], errors='coerce')\n",
    "    print('nunique:',df_doc['messageid'].nunique())\n",
    "    print('shape:',df_doc.shape[0])\n",
    "    assert df_doc['messageid'].nunique()==df_doc.shape[0]\n",
    "    print('Converted data into structured format successfully')\n",
    "\n",
    "    engine = create_engine(ps_info)\n",
    "    df_doc.to_sql('stg_email_table', engine, if_exists='replace', index=False)\n",
    "    print('Dump data into postgres staging table successfully')\n",
    "    \n",
    "    upsert_query = \"\"\"\n",
    "    INSERT INTO email_table (messageid, \"date\", \"from\", \"to\", subject, cc, bcc, xfrom, xto, xcc, xbcc, xfolder, xorigin, xfilename, body)\n",
    "    SELECT messageid, \"date\", \"from\", \"to\", subject, cc, bcc, xfrom, xto, xcc, xbcc, xfolder, xorigin, xfilename, body\n",
    "    FROM stg_email_table\n",
    "    ON CONFLICT (messageid)\n",
    "    DO UPDATE SET\n",
    "        \"date\" = EXCLUDED.\"date\",\n",
    "        \"from\" = EXCLUDED.\"from\",\n",
    "        \"to\" = EXCLUDED.\"to\",\n",
    "        subject = EXCLUDED.subject,\n",
    "        cc = EXCLUDED.cc,\n",
    "        bcc = EXCLUDED.bcc,\n",
    "        xfrom = EXCLUDED.xfrom,\n",
    "        xto = EXCLUDED.xto,\n",
    "        xcc = EXCLUDED.xcc,\n",
    "        xbcc = EXCLUDED.xbcc,\n",
    "        xfolder = EXCLUDED.xfolder,\n",
    "        xorigin = EXCLUDED.xorigin,\n",
    "        xfilename = EXCLUDED.xfilename,\n",
    "        body = EXCLUDED.body;\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    with engine.connect() as connection:\n",
    "        transaction = connection.begin()  # Start a transaction\n",
    "        try:\n",
    "            connection.execute(text(upsert_query))  # Execute the upsert query\n",
    "            transaction.commit()  # Commit the transaction if no error occurs\n",
    "            print(\"Updated postgres target table successfully.\")\n",
    "        except Exception as e:\n",
    "            transaction.rollback()  # Rollback the transaction if an error occurs\n",
    "            print(f\"Error: {e}\")\n",
    "    \n",
    "    with engine.connect() as connection:\n",
    "        transaction = connection.begin()  # Start a transaction\n",
    "        try:\n",
    "            connection.execute(text(\"TRUNCATE TABLE stg_email_table\"))\n",
    "            transaction.commit()  # Commit the transaction if no error occurs\n",
    "            print(\"Truncated postgres staging table successfully\")\n",
    "        except Exception as e:\n",
    "            transaction.rollback()  # Rollback the transaction if an error occurs\n",
    "            print(f\"Error: {e}\")\n",
    "            \n",
    "def postgres_job_v2(mongo_info, ps_info):\n",
    "    \n",
    "    client = MongoClient(mongo_info['client'])\n",
    "    db = client[mongo_info['db']] \n",
    "    collection = db[mongo_info['coll']]\n",
    "    print('Accessed mongo successfully')\n",
    "    \n",
    "    documents = collection.find()\n",
    "    documents = list(documents)\n",
    "    print('Extracted from mongo successfully')\n",
    "    \n",
    "    df_doc = pd.DataFrame(documents)\n",
    "    df_doc.drop_duplicates('file',keep='first',inplace=True)\n",
    "    df_doc = df_doc['message'].apply(parse_email).apply(pd.Series)\n",
    "    df_doc.columns = normalize_column_names(df_doc.columns)\n",
    "    df_doc['date'] = pd.to_datetime(df_doc['date'], errors='coerce')\n",
    "    print('nunique:',df_doc['messageid'].nunique())\n",
    "    print('shape:',df_doc.shape[0])\n",
    "    assert df_doc['messageid'].nunique()==df_doc.shape[0]\n",
    "    print('Converted data into structured format successfully')\n",
    "\n",
    "    engine = create_engine(ps_info)\n",
    "    metadata = MetaData()\n",
    "    table = Table('email_table', metadata, autoload_with=engine)\n",
    "\n",
    "    insert_stmt = insert(table).values(df_doc.to_dict(orient='records'))\n",
    "    do_nothing_stmt = insert_stmt.on_conflict_do_nothing(index_elements=['messageid']) \n",
    "    \n",
    "    with engine.connect() as connection:\n",
    "        transaction = connection.begin()  # Start a transaction\n",
    "        try:\n",
    "            connection.execute(do_nothing_stmt)  # Execute the upsert query\n",
    "            transaction.commit()  # Commit the transaction if no error occurs\n",
    "            print(\"Updated postgres target table successfully.\")\n",
    "        except Exception as e:\n",
    "            transaction.rollback()  # Rollback the transaction if an error occurs\n",
    "            print(f\"Error: {e}\")\n",
    "            \n",
    "    connection.close()\n",
    "    client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessed mongo successfully\n",
      "Extracted from mongo successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/px/q4nr_g2d25gdcpxjdm5b63m00000gn/T/ipykernel_56455/2092399882.py:14: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_doc['date'] = pd.to_datetime(df_doc['date'], errors='coerce')\n",
      "/var/folders/px/q4nr_g2d25gdcpxjdm5b63m00000gn/T/ipykernel_56455/2092399882.py:14: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  df_doc['date'] = pd.to_datetime(df_doc['date'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nunique: 3152\n",
      "shape: 3152\n",
      "Converted data into structured format successfully\n",
      "Updated postgres target table successfully.\n"
     ]
    }
   ],
   "source": [
    "client = MongoClient(mongo_info['client'])\n",
    "db = client[mongo_info['db']] \n",
    "collection = db[mongo_info['coll']]\n",
    "print('Accessed mongo successfully')\n",
    "\n",
    "documents = collection.find()\n",
    "documents = list(documents)\n",
    "print('Extracted from mongo successfully')\n",
    "\n",
    "df_doc = pd.DataFrame(documents)\n",
    "df_doc.drop_duplicates('file',keep='first',inplace=True)\n",
    "df_doc = df_doc['message'].apply(parse_email).apply(pd.Series)\n",
    "df_doc.columns = normalize_column_names(df_doc.columns)\n",
    "df_doc['date'] = pd.to_datetime(df_doc['date'], errors='coerce')\n",
    "print('nunique:',df_doc['messageid'].nunique())\n",
    "print('shape:',df_doc.shape[0])\n",
    "assert df_doc['messageid'].nunique()==df_doc.shape[0]\n",
    "print('Converted data into structured format successfully')\n",
    "\n",
    "engine = create_engine(ps_info)\n",
    "metadata = MetaData()\n",
    "table = Table('email_table', metadata, autoload_with=engine)\n",
    "\n",
    "insert_stmt = insert(table).values(df_doc.to_dict(orient='records'))\n",
    "do_nothing_stmt = insert_stmt.on_conflict_do_nothing(index_elements=['messageid']) \n",
    "\n",
    "with engine.connect() as connection:\n",
    "    transaction = connection.begin()  # Start a transaction\n",
    "    try:\n",
    "        connection.execute(do_nothing_stmt)  # Execute the upsert query\n",
    "        transaction.commit()  # Commit the transaction if no error occurs\n",
    "        print(\"Updated postgres target table successfully.\")\n",
    "    except Exception as e:\n",
    "        transaction.rollback()  # Rollback the transaction if an error occurs\n",
    "        print(f\"Error: {e}\")\n",
    "        \n",
    "connection.close()\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = 'https://raw.githubusercontent.com/tnhanh/data-midterm-17A/refs/heads/main/email.csv'\n",
    "batch_size = 200\n",
    "mongo_info = {'client':'mongodb://root:example@localhost:27017/',\n",
    "               'db':'lab_data_mongo',\n",
    "               'coll':'email_raw'\n",
    "          }\n",
    "ps_info = 'postgresql+psycopg2://airflow:airflow@localhost:5432/lab_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read and converted csv successfully\n",
      "Load data into mongo successfully\n",
      "Accessed mongo successfully\n",
      "Extracted from mongo successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/px/q4nr_g2d25gdcpxjdm5b63m00000gn/T/ipykernel_56455/429659876.py:133: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_doc['date'] = pd.to_datetime(df_doc['date'], errors='coerce')\n",
      "/var/folders/px/q4nr_g2d25gdcpxjdm5b63m00000gn/T/ipykernel_56455/429659876.py:133: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  df_doc['date'] = pd.to_datetime(df_doc['date'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nunique: 3329\n",
      "shape: 3329\n",
      "Converted data into structured format successfully\n",
      "Updated postgres target table successfully.\n"
     ]
    }
   ],
   "source": [
    "mongo_job(file_dir, batch_size, mongo_info)\n",
    "postgres_job_v2(mongo_info, ps_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
